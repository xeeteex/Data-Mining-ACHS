{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRTKlqHHugjd0Pi0xmCkbP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xeeteex/Data-Mining-ACHS/blob/main/dw%26dm_lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Lab 2\n",
        "\n",
        "### Apriori algorithm:\n",
        "\n",
        "- Find frequently occurring itemsets using the Apriori algorithm.\n",
        "- Compute the support of the frequent itemset.\n",
        "- Compute the confidence and lift of an association rule.\n",
        "\n",
        "### FP-Growth algorithm.\n",
        "\n",
        "- Find frequently occurring itemsets using the FP-Growth algorithm.\n",
        "- Compute the support of the frequent itemset.\n",
        "-Compute the confidence and lift of an association rule.\n",
        "\n",
        "### Compare Apriori and FP-growth algorithms.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y7dAlVJVmeo8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWlI6SsWmIF6",
        "outputId": "ca2b1ba2-3fdf-4e90-a02c-bcd3afcc5a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.11/dist-packages (0.23.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mlxtend pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports for association rule mining"
      ],
      "metadata": {
        "id": "qKNy_1eOnRRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
        "import time"
      ],
      "metadata": {
        "id": "LL7HrpO0nBqk"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variables for file paths and thresholds."
      ],
      "metadata": {
        "id": "BcEoiwqNnhKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = {\n",
        "    'space.txt': '/content/space.txt',\n",
        "    'sports.txt': '/content/sports.txt'\n",
        "}\n",
        "min_support = 0.15\n",
        "min_confidence = 0.01"
      ],
      "metadata": {
        "id": "gIkGjSyVnXWz"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apriori algorithm implementation"
      ],
      "metadata": {
        "id": "FmXFgS27qCGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_support(itemset, transactions):\n",
        "    count = sum(1 for tx in transactions if itemset.issubset(set(tx)))\n",
        "    return count / len(transactions)\n",
        "\n",
        "def apriori(transactions, min_support):\n",
        "    total_tx = len(transactions)\n",
        "    item_counts = defaultdict(int)\n",
        "\n",
        "    for tx in transactions:\n",
        "        for item in tx:\n",
        "            item_counts[frozenset([item])] += 1\n",
        "\n",
        "    frequent_itemsets = {item: count for item, count in item_counts.items() if count / total_tx >= min_support}\n",
        "    all_frequent = frequent_itemsets.copy()\n",
        "    current_freq = list(frequent_itemsets.keys())\n",
        "    k = 2\n",
        "\n",
        "    while current_freq:\n",
        "        candidates = set()\n",
        "        for i in range(len(current_freq)):\n",
        "            for j in range(i + 1, len(current_freq)):\n",
        "                union = current_freq[i] | current_freq[j]\n",
        "                if len(union) == k:\n",
        "                    candidates.add(union)\n",
        "\n",
        "        candidate_counts = defaultdict(int)\n",
        "        for tx in transactions:\n",
        "            tx_set = set(tx)\n",
        "            for candidate in candidates:\n",
        "                if candidate.issubset(tx_set):\n",
        "                    candidate_counts[candidate] += 1\n",
        "\n",
        "        current_freq = [item for item in candidate_counts if candidate_counts[item] / total_tx >= min_support]\n",
        "        all_frequent.update({item: candidate_counts[item] for item in current_freq})\n",
        "        k += 1\n",
        "\n",
        "    return all_frequent"
      ],
      "metadata": {
        "id": "n9cBsR6FnkRb"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code for generating association rules"
      ],
      "metadata": {
        "id": "lnMGc6nEqW15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_rules(frequent_itemsets, transactions, min_confidence):\n",
        "    total_tx = len(transactions)\n",
        "    rules = []\n",
        "    for itemset, support_count_itemset in frequent_itemsets.items():\n",
        "        if len(itemset) < 2:\n",
        "            continue\n",
        "        support_itemset = support_count_itemset / total_tx\n",
        "        for i in range(1, len(itemset)):\n",
        "            for antecedent in combinations(itemset, i):\n",
        "                antecedent = frozenset(antecedent)\n",
        "                consequent = itemset - antecedent\n",
        "\n",
        "                # Get support of antecedent from frequent_itemsets if available,\n",
        "                # otherwise calculate from transactions (should be infrequent if not in frequent_itemsets)\n",
        "                support_ante = frequent_itemsets.get(antecedent, 0) / total_tx\n",
        "\n",
        "                if support_ante == 0: # Avoid division by zero if antecedent support is 0\n",
        "                    continue\n",
        "\n",
        "                support_cons = get_support(consequent, transactions) # Still need to calculate consequent support\n",
        "\n",
        "                confidence = support_itemset / support_ante\n",
        "                lift = confidence / support_cons if support_cons > 0 else float('inf') # Handle division by zero for lift\n",
        "\n",
        "                if confidence >= min_confidence:\n",
        "                    rules.append({\n",
        "                        'antecedents': set(antecedent),\n",
        "                        'consequents': set(consequent),\n",
        "                        'support': round(support_itemset, 2),\n",
        "                        'confidence': round(confidence, 2),\n",
        "                        'lift': round(lift, 2)\n",
        "                    })\n",
        "    return rules"
      ],
      "metadata": {
        "id": "g-wGiNyhqHix"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FfeWLmFIqYF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, path in file_paths.items():\n",
        "    print(f\"\\n===== Processing {name} =====\")\n",
        "\n",
        "    transactions = []\n",
        "    with open(path, 'r') as file:\n",
        "        next(file)\n",
        "        for line in file:\n",
        "            parts = line.strip().split(',')\n",
        "            transactions.append([item.strip() for item in parts[1:] if item.strip()])\n",
        "\n",
        "    frequent_itemsets_raw = apriori(transactions, min_support)\n",
        "    rules = generate_rules(frequent_itemsets_raw, transactions, min_confidence)\n",
        "\n",
        "    total_tx = len(transactions)\n",
        "    frequent_itemsets_df = pd.DataFrame([{\n",
        "        'itemsets': set(item),\n",
        "        'support': round(count / total_tx, 2)\n",
        "    } for item, count in frequent_itemsets_raw.items()])\n",
        "\n",
        "    rules_df = pd.DataFrame(rules)\n",
        "\n",
        "    print(\"\\nFrequent Itemsets:\\n\", frequent_itemsets_df)\n",
        "\n",
        "    if not rules_df.empty:\n",
        "        print(\"\\nAssociation Rules:\\n\", rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "    else:\n",
        "        print(\"\\nNo association rules found with confidence ≥\", min_confidence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufCLfKVGqgnY",
        "outputId": "14c11e73-3071-4c0c-90d0-b25fc5bfc5b2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Processing space.txt =====\n",
            "\n",
            "Frequent Itemsets:\n",
            "                      itemsets  support\n",
            "0               {Robotic Arm}     0.33\n",
            "1              {Food Packets}     0.39\n",
            "2              {Sleeping Bag}     0.31\n",
            "3                 {Treadmill}     0.27\n",
            "4                {Space Suit}     0.31\n",
            "5                {3D Printer}     0.27\n",
            "6  {Carbon Dioxide Scrubbers}     0.24\n",
            "\n",
            "No association rules found with confidence ≥ 0.01\n",
            "\n",
            "===== Processing sports.txt =====\n",
            "\n",
            "Frequent Itemsets:\n",
            "          itemsets  support\n",
            "0      {football}     0.43\n",
            "1  {cricket ball}     0.35\n",
            "2        {gloves}     0.35\n",
            "3   {cricket bat}     0.39\n",
            "4         {juice}     0.41\n",
            "5  {water bottle}     0.27\n",
            "6     {ice cream}     0.25\n",
            "\n",
            "No association rules found with confidence ≥ 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "df_fp = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "fp_itemsets = fpgrowth(df_fp, min_support=min_support, use_colnames=True)\n",
        "\n",
        "fp_rules = association_rules(fp_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
        "\n",
        "print(\"\\nFP-Growth Frequent Itemsets:\\n\", fp_itemsets)\n",
        "if not fp_rules.empty:\n",
        "    print(\"\\nFP-Growth Association Rules:\\n\", fp_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "else:\n",
        "    print(\"\\nNo association rules found using FP-Growth with confidence ≥\", min_confidence)\n",
        "\n",
        "\n",
        "start_apriori = time.time()\n",
        "frequent_itemsets_raw = apriori(transactions, min_support)\n",
        "rules = generate_rules(frequent_itemsets_raw, transactions, min_confidence)\n",
        "end_apriori = time.time()\n",
        "\n",
        "start_fp = time.time()\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(transactions).transform(transactions)\n",
        "df_fp = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "fp_itemsets = fpgrowth(df_fp, min_support=min_support, use_colnames=True)\n",
        "fp_rules = association_rules(fp_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
        "end_fp = time.time()\n",
        "\n",
        "print(f\"\\nExecution Time (Apriori): {round(end_apriori - start_apriori, 4)} seconds\")\n",
        "print(f\"Execution Time (FP-Growth): {round(end_fp - start_fp, 4)} seconds\")\n",
        "\n",
        "\n",
        "print(\"\\n=== Comparison Summary ===\")\n",
        "print(f\"Apriori generated {len(rules)} rules\")\n",
        "print(f\"FP-Growth generated {len(fp_rules)} rules\")\n",
        "if (end_apriori - start_apriori) > (end_fp - start_fp):\n",
        "    print(\"FP-Growth is faster than Apriori.\")\n",
        "else:\n",
        "    print(\"Apriori is faster than FP-Growth.\")\n",
        "print(\"Both algorithms generated similar types of association rules, but FP-Growth is generally more efficient for large datasets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtQAQR8TqrrB",
        "outputId": "701217a3-5518-4c0c-a784-701bb3afb86d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FP-Growth Frequent Itemsets:\n",
            "     support        itemsets\n",
            "0  0.431373      (football)\n",
            "1  0.352941        (gloves)\n",
            "2  0.352941  (cricket ball)\n",
            "3  0.411765         (juice)\n",
            "4  0.392157   (cricket bat)\n",
            "5  0.274510  (water bottle)\n",
            "6  0.254902     (ice cream)\n",
            "\n",
            "No association rules found using FP-Growth with confidence ≥ 0.01\n",
            "\n",
            "Execution Time (Apriori): 0.0005 seconds\n",
            "Execution Time (FP-Growth): 0.0062 seconds\n",
            "\n",
            "=== Comparison Summary ===\n",
            "Apriori generated 0 rules\n",
            "FP-Growth generated 0 rules\n",
            "Apriori is faster than FP-Growth.\n",
            "Both algorithms generated similar types of association rules, but FP-Growth is generally more efficient for large datasets.\n"
          ]
        }
      ]
    }
  ]
}